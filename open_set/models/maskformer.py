# Copyright (c) OpenMMLab. All rights reserved.
import copy

import mmcv
import numpy as np
import torch
from typing import Dict, List, Union, Tuple

from mmdet.core import INSTANCE_OFFSET, bbox2result
from ..core.visualization import imshow_det_bboxes
from mmdet.models.builder import DETECTORS, build_backbone, build_head, build_neck
from mmdet.models.detectors.single_stage import SingleStageDetector
from mmdet.core.mask.structures import BitmapMasks


@DETECTORS.register_module()
class MaskFormerOpen(SingleStageDetector):
    r"""Implementation of `Per-Pixel Classification is
    NOT All You Need for Semantic Segmentation
    <https://arxiv.org/pdf/2107.06278>`_."""

    def __init__(self,
                 backbone,
                 neck=None,
                 panoptic_head=None,
                 panoptic_fusion_head=None,
                 train_cfg=None,
                 test_cfg=None,
                 init_cfg=None):
        super(SingleStageDetector, self).__init__(init_cfg=init_cfg)
        self.backbone = build_backbone(backbone)
        if neck is not None:
            self.neck = build_neck(neck)

        panoptic_head_ = copy.deepcopy(panoptic_head)
        panoptic_head_.update(train_cfg=train_cfg)
        panoptic_head_.update(test_cfg=test_cfg)
        self.panoptic_head = build_head(panoptic_head_)

        panoptic_fusion_head_ = copy.deepcopy(panoptic_fusion_head)
        panoptic_fusion_head_.update(test_cfg=test_cfg)
        self.panoptic_fusion_head = build_head(panoptic_fusion_head_)

        self.num_things_classes = self.panoptic_fusion_head.num_things_classes
        self.num_stuff_classes = self.panoptic_fusion_head.num_stuff_classes
        self.num_classes = self.panoptic_fusion_head.num_classes

        self.train_cfg = train_cfg
        self.test_cfg = test_cfg

        # BaseDetector.show_result default for instance segmentation
        if self.num_stuff_classes > 0:
            self.show_result = self._show_pan_result

    def forward_dummy(self, img: torch.Tensor):
        """Used for computing network flops. See
        `mmdetection/tools/analysis_tools/get_flops.py`

        Args:
            img: An image shape (N, C, H, W) encoding input images.
                Typically these should be mean centered and std scaled.
        """
        img_metas = [{'img_shape': [1280, 800]}]
        super(SingleStageDetector, self).forward_train(img, img_metas)
        x = self.extract_feat(img)
        outs = self.panoptic_head(x, img_metas)
        # caption generator
        gt_caption_embs = torch.randn([1, 35, 768]).to(img.device)
        gt_caption_masks = torch.ones([1, 35]).bool().to(img.device)
        cls_emb_preds = torch.randn([1, 100, 768]).to(img.device)
        caption_logits = self.panoptic_head.caption_generator(
            tgt=gt_caption_embs[:, :-1, :],
            memory=cls_emb_preds,
            tgt_key_padding_mask=torch.logical_not(gt_caption_masks[:, :-1]))[1]
        return outs

    def forward_train(self,
                      img: torch.Tensor,
                      img_metas: List[Dict],
                      gt_bboxes: List[torch.Tensor],
                      gt_labels: List[torch.Tensor],
                      gt_masks: List[BitmapMasks],
                      gt_caption_ids: List[torch.Tensor] =None,
                      gt_caption_mask: List[torch.Tensor] =None,
                      gt_caption_nouns_ids: List[torch.Tensor]=None,
                      gt_caption_nouns_mask: List[torch.Tensor]=None,
                      gt_semantic_seg=None,
                      gt_bboxes_ignore=None,
                      **kwargs) -> Dict[str, torch.Tensor]:
        """Performs a single training step.
        
        Args:
            img: A tensor of shape (B, C, H, W) encoding input images. Typically these should be mean centered and std scaled.
            img_metas: A list of image info dict where each dict has: 'img_shape', 'scale_factor', 'flip', and may also contain
                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'. For details on the values of these keys see
                `mmdet/datasets/pipelines/formatting.py:Collect`.
            gt_bboxes: Ground truth bboxes for each image with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
            gt_labels: Class indices corresponding to the boxes.
            gt_masks: A list of segmentation masks for each box used if the architecture supports a segmentation task.
            gt_caption_ids: A list of token ids for the caption words.
            gt_caption_mask: A list of masks for the caption words.
            gt_caption_nouns_ids: A list of token ids for the nouns in the caption.
            gt_caption_nouns_mask: A list of maks for the nouns in the caption.
            kwargs:
                gt_cat_names (list[list[str]]): List of List of category names
                of the corresponding label in gt_labels

        Returns:
            A dictionary of loss components
            
        Reference:
            https://mmdetection.readthedocs.io/en/latest/user_guides/config.html
        """
        # add batch_input_shape in img_metas
        super(SingleStageDetector, self).forward_train(img, img_metas)
        
        x = self.extract_feat(img)
        losses = self.panoptic_head.forward_train(x, img_metas, gt_bboxes, gt_labels, gt_masks, gt_semantic_seg, gt_caption_ids,
                                                  gt_caption_mask, gt_caption_nouns_ids, gt_caption_nouns_mask, gt_bboxes_ignore, **kwargs)

        return losses

    def simple_test(self, imgs: torch.Tensor, img_metas, **kwargs) -> List[Dict[str, Tuple[List[np.ndarray], List[List[np.ndarray]]]]]:
        """Test without augmentation, often used for inference

        Args:
            imgs: A batch of images.
            img_metas: List of image information.
            **kwargs (list[list[Tensor]]): In visualization mode, gt_labels, gt_masks and
                other annotations

        Returns:
            Semantic segmentation results and panoptic segmentation \
            results of each image for panoptic segmentation, or formatted \
            bbox and mask results of each image for instance segmentation.
            Each item in the list is a dictionary. 1 dictionary for 1 type of result.
        """
        feats = self.extract_feat(imgs)
        assigned_labels, mask_cls_emb_results, mask_pred_results, caption_generation_results, att = \
            self.panoptic_head.simple_test(feats, img_metas, **kwargs)
        
        results = self.panoptic_fusion_head.simple_test(
            assigned_labels, mask_cls_emb_results, mask_pred_results, img_metas, **kwargs)

        for i in range(len(results)):
            for res_type in self.test_cfg.get('eval_types', []):
                if res_type == 'all_results':
                    pred_classes = self.panoptic_fusion_head.all_classes
                elif res_type == 'novel_results':
                    pred_classes = self.panoptic_fusion_head.novel_classes
                elif res_type == 'base_results':
                    pred_classes = self.panoptic_fusion_head.base_classes
                elif res_type == 'ins_results':
                    pred_classes = self.panoptic_fusion_head.num_classes
                elif res_type == 'cap_results':
                    results[i][res_type] = caption_generation_results
                    continue

                # panoptic mode reuslts
                if 'pan' in list(results[i].keys())[0]:
                    type = list(results[i].keys())[0]
                    results[i][type] = results[i][type].detach(
                    ).cpu().numpy()
                else:
                    if res_type in results[i]:
                        labels_per_image, bboxes, mask_pred_binary = results[i][res_type]
                        bbox_results: List[np.ndarray] = bbox2result(bboxes, labels_per_image,
                                                    pred_classes)
                        mask_results = [[] for _ in range(pred_classes)]
                        for j, label in enumerate(labels_per_image):
                            mask = mask_pred_binary[j].detach().cpu().numpy()
                            mask_results[label].append(mask)
                        results[i][res_type] = bbox_results, mask_results
                    elif 'grounding' in results[i]:
                        _, bbox_results, mask_pred_binary = results[i]['grounding']
                        bbox_results = [bbox_results.detach().cpu().numpy()]
                        mask_results = [[mask.detach().cpu().numpy() for mask in mask_pred_binary]]
                        results[i]['grounding'] = bbox_results, mask_results   

            if kwargs.get('with_mask', False):
                results[i]['mask'] = mask_pred_results.cpu().numpy()
            if kwargs.get('with_att', False):
                results[i]['att'] = att.cpu().numpy()
            if kwargs.get('gt_labels', None) is not None:
                # (num_queries, d_l) (num_queries, )
                results[i]['visual'] = mask_cls_emb_results.squeeze().cpu().numpy(), assigned_labels.cpu().numpy() 
            
        return results

    def aug_test(self, imgs, img_metas, **kwargs):
        raise NotImplementedError

    def onnx_export(self, img, img_metas):
        raise NotImplementedError(f'{self.__class__.__name__} does '
                                  f'not support ONNX EXPORT')

    def show_result(self,
                    img,
                    result,
                    score_thr=0.3,
                    bbox_color=(72, 101, 241),
                    text_color=(72, 101, 241),
                    mask_color=None,
                    thickness=2,
                    font_size=13,
                    win_name='',
                    show=False,
                    wait_time=0,
                    out_file=None):
        """Draw `result` over `img`.

        Args:
            img (str or Tensor): The image to be displayed.
            result (Tensor or tuple): The results to draw over `img`
                bbox_result or (bbox_result, segm_result).
            score_thr (float, optional): Minimum score of bboxes to be shown.
                Default: 0.3.
            bbox_color (str or tuple(int) or :obj:`Color`):Color of bbox lines.
               The tuple of color should be in BGR order. Default: 'green'
            text_color (str or tuple(int) or :obj:`Color`):Color of texts.
               The tuple of color should be in BGR order. Default: 'green'
            mask_color (None or str or tuple(int) or :obj:`Color`):
               Color of masks. The tuple of color should be in BGR order.
               Default: None
            thickness (int): Thickness of lines. Default: 2
            font_size (int): Font size of texts. Default: 13
            win_name (str): The window name. Default: ''
            wait_time (float): Value of waitKey param.
                Default: 0.
            show (bool): Whether to show the image.
                Default: False.
            out_file (str or None): The filename to write the image.
                Default: None.

        Returns:
            img (Tensor): Only if not `show` or `out_file`
        """
        img = mmcv.imread(img)
        img = img.copy()
        if 'all_results' in result:
            all_result = result['all_results']
        elif 'grounding' in result:
            all_result = result['grounding']
        else:
            all_result = result['base_results']
        bbox_result, segm_result = all_result
        bboxes = np.vstack(bbox_result)
        labels = [
            np.full(bbox.shape[0], i, dtype=np.int32)
            for i, bbox in enumerate(bbox_result)
        ]
        labels = np.concatenate(labels)
        # draw segmentation masks
        segms = None
        if segm_result is not None and len(labels) > 0:  # non empty
            segms = mmcv.concat_list(segm_result)
            if isinstance(segms[0], torch.Tensor):
                segms = torch.stack(segms, dim=0).detach().cpu().numpy()
            else:
                segms = np.stack(segms, axis=0)
        # if out_file specified, do not show image in window
        if out_file is not None:
            show = False
        
        # draw bounding boxes
        img = imshow_det_bboxes(
            img,
            bboxes,
            labels,
            segms,
            class_names=self.CLASSES if 'grounding' not in result else ('',),
            score_thr=score_thr,
            bbox_color=bbox_color,
            text_color=text_color,
            mask_color=mask_color,
            thickness=thickness,
            font_size=font_size,
            win_name=win_name,
            show=show,
            wait_time=wait_time,
            out_file=out_file)

        if not (show or out_file):
            return img

    def _show_pan_result(self,
                         img,
                         result,
                         score_thr=0.3,
                         bbox_color=(72, 101, 241),
                         text_color=(72, 101, 241),
                         mask_color=None,
                         thickness=2,
                         font_size=20,
                         win_name='',
                         show=False,
                         wait_time=0,
                         out_file=None):
        """Draw `panoptic result` over `img`.

        Args:
            img (str or Tensor): The image to be displayed.
            result (dict): The results.

            score_thr (float, optional): Minimum score of bboxes to be shown.
                Default: 0.3.
            bbox_color (str or tuple(int) or :obj:`Color`):Color of bbox lines.
               The tuple of color should be in BGR order. Default: 'green'.
            text_color (str or tuple(int) or :obj:`Color`):Color of texts.
               The tuple of color should be in BGR order. Default: 'green'.
            mask_color (None or str or tuple(int) or :obj:`Color`):
               Color of masks. The tuple of color should be in BGR order.
               Default: None.
            thickness (int): Thickness of lines. Default: 2.
            font_size (int): Font size of texts. Default: 13.
            win_name (str): The window name. Default: ''.
            wait_time (float): Value of waitKey param.
                Default: 0.
            show (bool): Whether to show the image.
                Default: False.
            out_file (str or None): The filename to write the image.
                Default: None.

        Returns:
            img (Tensor): Only if not `show` or `out_file`.
        """
        img = mmcv.imread(img)
        img = img.copy()
        pan_results = result['panoptic_all_results']
        # keep objects ahead
        ids = np.unique(pan_results)[::-1]
        legal_indices = ids != self.num_classes  # for VOID label
        ids = ids[legal_indices]
        labels = np.array([id % INSTANCE_OFFSET for id in ids], dtype=np.int64)
        segms = (pan_results[None] == ids[:, None, None])

        # if out_file specified, do not show image in window
        if out_file is not None:
            show = False
        
        # draw bounding boxes
        img = imshow_det_bboxes(
            img,
            segms=segms,
            labels=labels,
            class_names=self.CLASSES,
            bbox_color=bbox_color,
            text_color=text_color,
            mask_color=mask_color,
            thickness=thickness,
            font_size=font_size,
            win_name=win_name,
            show=show,
            wait_time=wait_time,
            out_file=out_file)

        if not (show or out_file):
            return img